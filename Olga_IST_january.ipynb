{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralogs genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Including libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import Applications\n",
    "from Bio.Blast.Applications import NcbiblastxCommandline\n",
    "import gzip \n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ftplib import FTP\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patoolib\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2  Data from NCBI for Blastp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_blastp(bacterium):\n",
    "    # diving to database\n",
    "    ftp = FTP('ftp.ncbi.nlm.nih.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('genomes/refseq/bacteria')\n",
    "    ftp.cwd(bacterium)\n",
    "    \n",
    "    domain_list = ['representative','reference','latest_assembly_versions']\n",
    "    for domain in domain_list:\n",
    "        if domain in ftp.nlst():\n",
    "            ftp.cwd(domain)\n",
    "            break\n",
    "            \n",
    "    current_strain = ftp.nlst()\n",
    "    ftp.cwd(current_strain[0])\n",
    "     \n",
    "    namefile = current_strain[0] + '_protein.faa.gz' # file, which i want\n",
    "    out = \"C://idea_projects_my//my_file.gz\" # place on my disk\n",
    "    \n",
    "    with open(out,'wb') as f:\n",
    "        ftp.retrbinary('RETR ' + namefile,f.write)\n",
    "        f_read = gzip.open(out,'rb')\n",
    "    \n",
    "    os.mkdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    patoolib.extract_archive(out,outdir = 'C:\\\\idea_projects_my\\\\scherichia')  \n",
    "    os.listdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    new_out = 'C:\\\\idea_projects_my\\\\scherichia'\n",
    "    for unarchived_file in os.listdir('C:\\\\idea_projects_my\\\\scherichia'):\n",
    "        with open('C:\\\\idea_projects_my\\\\scherichia' + '\\\\' + unarchived_file, 'r') as myfile:\n",
    "            data = myfile.read()\n",
    "    #remove directories       \n",
    "    os.remove('C:\\\\idea_projects_my\\\\scherichia\\\\my_file')\n",
    "    os.rmdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(data):\n",
    "    names_of_genes = data.split(sep = '>WP_')\n",
    "    \n",
    "    return names_of_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_XML_and_holder(xml_current_file,gene_1):\n",
    "    name_of_gene,length,identity,e_value,gen = list(),list(),list(),list(),list()\n",
    "    with open('docu.xml','w') as file:\n",
    "        file.write(str(xml_current_file))\n",
    "        \n",
    "    root = ET.parse('docu.xml').getroot()\n",
    "    for subtags in root.iter('Iteration_hits'):\n",
    "        for subsubtags in subtags:\n",
    "            for objects in subsubtags:\n",
    "                if(objects.tag == 'Hit_def'):\n",
    "                    name_of_gene_cur = objects.text\n",
    "                if(objects.tag == 'Hit_len'):\n",
    "                    length_cur = int(objects.text)\n",
    "                if(objects.tag == 'Hit_hsps'):\n",
    "                    for tags in objects[0]:\n",
    "                        if(tags.tag == 'Hsp_evalue'):\n",
    "                            e_value_cur = float(tags.text)\n",
    "                        if(tags.tag == 'Hsp_identity'):\n",
    "                            identity_cur = int(tags.text)\n",
    "                            if(identity_cur/length_cur > 0.5 and identity_cur/length_cur != 1):\n",
    "                                gen.append(gene_1)\n",
    "                                name_of_gene.append(name_of_gene_cur)\n",
    "                                length.append(length_cur)\n",
    "                                e_value.append(e_value_cur)\n",
    "                                identity.append(float(identity_cur/length_cur))\n",
    "                        \n",
    "    return name_of_gene,length,identity,e_value,gen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 writing genes for files and Blastp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_program(names_of_genes,bacterium):\n",
    "    \n",
    "    gen,gene_2,length,identity,e_value,bact = list(),list(),list(),list(),list(),list()\n",
    "    #ways for genes\n",
    "    out_sample = 'C://idea_projects_my//Escherichia_sample'\n",
    "    out_database = 'C://idea_projects_my//Escherichia_database'\n",
    "    \n",
    "    for i, sample in tqdm(enumerate(names_of_genes)):\n",
    "        database = names_of_genes[i:]\n",
    "        try:\n",
    "            with open(out_sample,'w')as file:\n",
    "                file.write(str(sample))\n",
    "            with open(out_database,'w')as file:\n",
    "                file.write(\"\".join(map(lambda x: \">WP_\" + x, database)))\n",
    "        except(FileNotFoundError,OSError):\n",
    "            print('problems with directories')\n",
    "            continue\n",
    "        \n",
    "        gene_1 = \"WP_\" + str(sample)[: str(sample).find(\" \")]\n",
    "        \n",
    "        !makeblastdb -in C://idea_projects_my//Escherichia_database -dbtype prot -out C://idea_projects_my//Escherichia_database      \n",
    "        \n",
    "        result = NcbiblastxCommandline(cmd='blastp',num_threads = 4,query= out_sample,\n",
    "                              db=out_database,evalue=1e-9,outfmt = 5)\n",
    "        \n",
    "        try:\n",
    "            #out, err = result()\n",
    "            output_of_xml = Parse_XML_and_holder(result()[0],gene_1)\n",
    "            gene_2.extend(output_of_xml[0])\n",
    "            length.extend(output_of_xml[1])\n",
    "            identity.extend(output_of_xml[2])\n",
    "            e_value.extend(output_of_xml[3])\n",
    "            gen.extend(output_of_xml[4])\n",
    "            \n",
    "        except:\n",
    "            print(\"oops\" + str(i))\n",
    "            continue\n",
    "            \n",
    "        \n",
    "    return gene_2,length,identity,e_value,bacterium,gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Searching placements of data for each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_placements(bacterium):\n",
    "    # diving to database\n",
    "    ftp = FTP('ftp.ncbi.nlm.nih.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('genomes/refseq/bacteria')\n",
    "    ftp.cwd(bacterium)\n",
    "    \n",
    "    domain_list = ['representative','reference','latest_assembly_versions']\n",
    "    for domain in domain_list:\n",
    "        if domain in ftp.nlst():\n",
    "            ftp.cwd(domain)\n",
    "            break\n",
    "            \n",
    "    current_strain = ftp.nlst()\n",
    "    ftp.cwd(current_strain[0])\n",
    "     \n",
    "    namefile = current_strain[0] + '_translated_cds.faa.gz' # file, which i want\n",
    "    out = \"C://idea_projects_my//my_file.gz\" # place on my disk\n",
    "    \n",
    "    with open(out,'wb') as f:\n",
    "        ftp.retrbinary('RETR ' + namefile,f.write)\n",
    "        f_read = gzip.open(out,'rb')\n",
    "    \n",
    "    os.mkdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    patoolib.extract_archive(out,outdir = 'C:\\\\idea_projects_my\\\\scherichia')  \n",
    "    os.listdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    new_out = 'C:\\\\idea_projects_my\\\\scherichia'\n",
    "    for unarchived_file in os.listdir('C:\\\\idea_projects_my\\\\scherichia'):\n",
    "        with open('C:\\\\idea_projects_my\\\\scherichia' + '\\\\' + unarchived_file, 'r') as myfile:\n",
    "            data = myfile.read()\n",
    "    #remove directories       \n",
    "    os.remove('C:\\\\idea_projects_my\\\\scherichia\\\\my_file')\n",
    "    os.rmdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting(data):\n",
    "    genes = data.split(sep = '>')\n",
    "    genes = genes[1:]\n",
    "    return genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def information_of_gene(gene,genes):\n",
    "    for element in genes:\n",
    "        if(gene in element):\n",
    "            return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deriving_number(query):\n",
    "    if('complement' in query):\n",
    "        s = re.findall(r'complement\\D\\w+',query)\n",
    "        number = int(re.findall(r'\\w+',str(s))[1])\n",
    "    else:\n",
    "        s = re.findall(r'location=\\w+',query)\n",
    "        number = int(re.findall(r'\\w+',str(s))[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Origin and Terminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "def detect_peaks(x, mph=None, mpd=1, threshold=0, edge='rising',\n",
    "                 kpsh=False, valley=False, show=False, ax=None):\n",
    "     x = np.atleast_1d(x).astype('float64')\n",
    "    if x.size < 3:\n",
    "        return np.array([], dtype=int)\n",
    "    if valley:\n",
    "        x = -x\n",
    "    # find indices of all peaks\n",
    "    dx = x[1:] - x[:-1]\n",
    "    # handle NaN's\n",
    "    indnan = np.where(np.isnan(x))[0]\n",
    "    if indnan.size:\n",
    "        x[indnan] = np.inf\n",
    "        dx[np.where(np.isnan(dx))[0]] = np.inf\n",
    "    ine, ire, ife = np.array([[], [], []], dtype=int)\n",
    "    if not edge:\n",
    "        ine = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "    else:\n",
    "        if edge.lower() in ['rising', 'both']:\n",
    "            ire = np.where((np.hstack((dx, 0)) <= 0) & (np.hstack((0, dx)) > 0))[0]\n",
    "        if edge.lower() in ['falling', 'both']:\n",
    "            ife = np.where((np.hstack((dx, 0)) < 0) & (np.hstack((0, dx)) >= 0))[0]\n",
    "    ind = np.unique(np.hstack((ine, ire, ife)))\n",
    "    # handle NaN's\n",
    "    if ind.size and indnan.size:\n",
    "        # NaN's and values close to NaN's cannot be peaks\n",
    "        ind = ind[np.in1d(ind, np.unique(np.hstack((indnan, indnan-1, indnan+1))), invert=True)]\n",
    "    # first and last values of x cannot be peaks\n",
    "    if ind.size and ind[0] == 0:\n",
    "        ind = ind[1:]\n",
    "    if ind.size and ind[-1] == x.size-1:\n",
    "        ind = ind[:-1]\n",
    "    # remove peaks < minimum peak height\n",
    "    if ind.size and mph is not None:\n",
    "        ind = ind[x[ind] >= mph]\n",
    "    # remove peaks - neighbors < threshold\n",
    "    if ind.size and threshold > 0:\n",
    "        dx = np.min(np.vstack([x[ind]-x[ind-1], x[ind]-x[ind+1]]), axis=0)\n",
    "        ind = np.delete(ind, np.where(dx < threshold)[0])\n",
    "    # detect small peaks closer than minimum peak distance\n",
    "    if ind.size and mpd > 1:\n",
    "        ind = ind[np.argsort(x[ind])][::-1]  # sort ind by peak height\n",
    "        idel = np.zeros(ind.size, dtype=bool)\n",
    "        for i in range(ind.size):\n",
    "            if not idel[i]:\n",
    "                # keep peaks with the same height if kpsh is True\n",
    "                idel = idel | (ind >= ind[i] - mpd) & (ind <= ind[i] + mpd) \\\n",
    "                    & (x[ind[i]] > x[ind] if kpsh else True)\n",
    "                idel[i] = 0  # Keep current peak\n",
    "        # remove the small peaks and sort back the indices by their occurrence\n",
    "        ind = np.sort(ind[~idel])\n",
    "\n",
    "    if show:\n",
    "        if indnan.size:\n",
    "            x[indnan] = np.nan\n",
    "        if valley:\n",
    "            x = -x\n",
    "        #_plot(x, mph, mpd, threshold, edge, valley, ax, ind)\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):\n",
    "    \"\"\"Plot results of the detect_peaks function, see its help\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print('matplotlib is not available.')\n",
    "    else:\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "        ax.plot(x, 'b', lw=1)\n",
    "        if ind.size:\n",
    "            label = 'valley' if valley else 'peak'\n",
    "            label = label + 's' if ind.size > 1 else label\n",
    "            ax.plot(ind, x[ind], '+', mfc=None, mec='r', mew=2, ms=8,\n",
    "                    label='%d %s' % (ind.size, label))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_GC_skew(seq):\n",
    "    \"\"\"Calculate the GC-skew for a given sequence\"\"\"\n",
    "    \n",
    "\n",
    "    values = []\n",
    "    window = int(0.000005 * len(seq))\n",
    "    # window = 25\n",
    "    for i in range(0, len(seq), window):\n",
    "        s = seq[i: i + window]\n",
    "        g = s.count('G') + s.count('g')\n",
    "        c = s.count('C') + s.count('c')\n",
    "        skew = (g-c)#/float(g+c)\n",
    "        values.append(skew)\n",
    "\n",
    "    \n",
    "    return values, window\n",
    "\n",
    "\n",
    "def smoothListGaussian(myarray, degree=5000):\n",
    "    \"\"\"Smooth out the cumulative skew values for re-plotting\"\"\"\n",
    "    myarray  = np.pad(myarray, (degree-1, degree-1), mode='edge')\n",
    "    window   = degree*2-1\n",
    "    weight   = np.arange(-degree+1, degree)/window\n",
    "    weight   = np.exp(-(16*weight**2))\n",
    "    weight  /= sum(weight)\n",
    "    smoothed = np.convolve(myarray, weight, mode='valid')\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_skew(skewArray):\n",
    "    \"\"\"Plots a skew diagram and returns the respective cumulative values\"\"\"\n",
    "\n",
    "    skewCumValues = np.cumsum(skewArray)\n",
    "    #fig = plt.figure()\n",
    "    #ax1 = fig.add_subplot(211)\n",
    "    #ax2 = fig.add_subplot(212)\n",
    "\n",
    "    ## ax1.plot(range(len(skewArray)), skewArray, 'r', color='blue')  # noise values\n",
    "    #ax1.plot(range(len(skewArray)), skewCumValues, 'r', color='red') # cumulative values\n",
    "\n",
    "    smoothedY = smoothListGaussian(skewCumValues)\n",
    "    #ax2.plot(range(len(smoothedY)), smoothedY, 'r', color='black')   # smooth values\n",
    "\n",
    "    #fig.savefig('SkewPlot.png')\n",
    "\n",
    "    \n",
    "    return(skewCumValues, smoothedY)\n",
    "\n",
    "\n",
    "def identifyPeaks(data, stepsize):\n",
    "    \"\"\"Identify peaks/valleys (global min/max) from the smoothed data\"\"\"\n",
    "    ind = detect_peaks(data, mpd=stepsize ,valley=True , show=True)\n",
    "    return (ind)\n",
    "\n",
    "def identifyPeaks_max(data,stepsize):\n",
    "    ind = detect_peaks(data,mpd = stepsize,valley = False,show = True)\n",
    "    return(ind)\n",
    "\n",
    "def identifyPeaks_min(data,stepsize):\n",
    "    ind = detect_peaks(data,mpd = stepsize,valley = True,show = True)\n",
    "    return (ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searching_of_maxim_and_minim(bacterium):\n",
    "    \n",
    "    \"\"\"\n",
    "    >>> searching_of_maxim_and_minim('Acaryochloris_marina')\n",
    "    (1129212, 3134586, 8466864, 42)\n",
    "    \n",
    "    >>> searching_of_maxim_and_minim('Acholeplasma_laidlawii')\n",
    "    (723331, 0, 1515749, 7)\n",
    "    \n",
    "    >>> searching_of_maxim_and_minim('Anaerolinea_thermophila')\n",
    "    (1708704, 3289534, 3576582, 17)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ftp = FTP('ftp.ncbi.nlm.nih.gov')\n",
    "    ftp.login()\n",
    "    ftp.cwd('genomes/refseq/bacteria')\n",
    "    ftp.cwd(bacterium)\n",
    "    \n",
    "    domain_list = ['representative','reference','latest_assembly_versions']\n",
    "    for domain in domain_list:\n",
    "        if domain in ftp.nlst():\n",
    "            ftp.cwd(domain)\n",
    "            break\n",
    "            \n",
    "    current_strain = ftp.nlst()\n",
    "    ftp.cwd(current_strain[0])\n",
    "     \n",
    "    namefile = current_strain[0] + '_genomic.fna.gz' # file, which i want\n",
    "    out = \"C://idea_projects_my//my_file.gz\" # plac\n",
    "    with open(out,'wb') as f:\n",
    "        ftp.retrbinary('RETR ' + namefile,f.write)\n",
    "        f_read = gzip.open(out,'rb')\n",
    "    \n",
    "    os.mkdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    patoolib.extract_archive(out,outdir = 'C:\\\\idea_projects_my\\\\scherichia')  \n",
    "    os.listdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    new_out = 'C:\\\\idea_projects_my\\\\scherichia'\n",
    "    for unarchived_file in os.listdir('C:\\\\idea_projects_my\\\\scherichia'):\n",
    "        with open('C:\\\\idea_projects_my\\\\scherichia' + '\\\\' + unarchived_file, 'r') as myfile:\n",
    "            data = myfile.read()\n",
    "    #remove directories       \n",
    "    os.remove('C:\\\\idea_projects_my\\\\scherichia\\\\my_file')\n",
    "    os.rmdir('C:\\\\idea_projects_my\\\\scherichia')\n",
    "    \n",
    "    \n",
    "            \n",
    "    counter = 0\n",
    "    for element in data:\n",
    "        counter+=1\n",
    "        if element in ['A','T']:\n",
    "            break\n",
    "    data = data[counter + 1:]\n",
    "    \n",
    "    length_of_sequence = len(data)\n",
    "    coefficient = int(len(data)/200000)\n",
    "    \n",
    "    skewArray,window = calc_GC_skew(data)\n",
    "    skewCumValues,smoothValues = plot_skew(skewArray)\n",
    "    stepsize = 500\n",
    "    identifiedPeaks_maxim = identifyPeaks_max(smoothValues,stepsize)\n",
    "    identifiedPeaks_minim = identifyPeaks_min(smoothValues,stepsize)   \n",
    "\n",
    "    prev_element = 0\n",
    "    output = 0\n",
    "\n",
    "    for element in identifiedPeaks_minim:\n",
    "        if smoothValues[element] < smoothValues[prev_element]:\n",
    "            output = element\n",
    "            prev_element = element\n",
    "    \n",
    "    our_minimum = output\n",
    "\n",
    "    prev_element = 0\n",
    "    output = 0\n",
    "\n",
    "    for element in identifiedPeaks_maxim:\n",
    "        if smoothValues[element] > smoothValues[prev_element]:\n",
    "            output = element\n",
    "            prev_element = element\n",
    "    \n",
    "    our_maximum = output\n",
    "    \n",
    "    our_maximum *= coefficient\n",
    "    our_minimum *= coefficient\n",
    "    \n",
    "    return our_maximum,our_minimum,length_of_sequence,coefficient\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sum = pd.DataFrame()\n",
    "df_bac = pd.read_csv('olga_november.csv') # names of our bacteria\n",
    "bacteria = list(df_bac['bact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for bacterium in tqdm(bacteria):\n",
    "    \n",
    "    data = data_for_blastp(bacterium)\n",
    "    names_of_genes = splitting(data)\n",
    "    final = base_program(names_of_genes,bacterium)\n",
    "    gene_1 = final[5]\n",
    "    f = lambda x: str(x)[:str(x).find(\" \")]\n",
    "    gene_2 = [f(x) for x in final[0]]\n",
    "    \n",
    "    gene_1 = [x[3:] for x in gene_1]\n",
    "    gene_2 = [x[3:] for x in gene_2]\n",
    "    \n",
    "    location_of_gene_1 = []\n",
    "    location_of_gene_2 = []\n",
    "    data = data_for_placements(bacterium)\n",
    "    res = splitting(data)\n",
    "    for gene in gene_1:\n",
    "        query = information_of_gene(gene,res)\n",
    "        try:\n",
    "            location_of_gene_1.append(deriving_number(query))\n",
    "        except:\n",
    "            location_of_gene_1.append(-1)\n",
    "            continue\n",
    "    for gene in gene_2:\n",
    "        query = information_of_gene(gene,res)\n",
    "        try:\n",
    "            location_of_gene_2.append(deriving_number(query))\n",
    "        except:\n",
    "            location_of_gene_2.append(-1)\n",
    "            continue\n",
    "            \n",
    "            \n",
    "    list_maxim = list()\n",
    "    list_minim = list()\n",
    "    length_of_sequence = list()\n",
    "    result = searching_of_maxim_and_minim(bacterium)\n",
    "    list_maxim.append(result[0])\n",
    "    list_minim.append(result[1])\n",
    "    length_of_sequence.append(result[2])\n",
    "    data = {\"bact\":final[4],\"gene_1\": gene_1, \"gene_2\" : gene_2,\"loc_1\":location_of_gene_1,\"loc_2\":location_of_gene_2,\n",
    "           \"identity\":final[2],\"length\":final[1],\"e_value\":final[3],\"origin\":list_minim[0],\"terminus\":list_maxim[0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    df_sum = pd.concat([df,df_sum])\n",
    "    df_sum.to_csv('olga_january.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
